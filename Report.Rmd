---
title: "Report"
author: "Rushabh Khara"
date: "2023-05-21"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: float
geometry: left=1cm,right=1cm,top=1.5cm,bottom=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
library(ISLR)
library(ggplot2)
library(GGally)
library(car)
library(gridExtra)
library(MASS)
library(Hmisc)
library(faraway)
library(mice)
library(caret)
library(gbm)
library(tree)
library(e1071)
library(glmnet)
library(randomForest)
library(visdat)
library(dlookr)
library(knitr)
library(kableExtra)
library(rpart)
library(rpart.plot)


setwd("D:\\University\\STAT3040\\FinalProject")

train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)

```

# Introduction

The forthcoming report will focus on an extensive statistical analysis and predictive modelling of housing price data from the fictitious city, Heart-Landitopia. Our goal will be to scrutinize two key variables, namely the price at which houses are advertised and the presence of a school in the vicinity. We aim to unpack the relationship between these variables and a variety of other important covariates, while adequately addressing and discussing inherent uncertainties in the data.

The data under consideration covers diverse parameters such as latitude, longitude, tax rate, presence of homeowners association, HVAC system availability, garage spaces, house view, house age, number of bathrooms, bedrooms, stories, size of the lot and the living space. All of these factors, while being individually unique, contribute collectively towards our two main variables of interest.

In order to derive actionable insights, firstly, we will deploy a minimum of five different classes of models to predict the price of a house based on other variables, where our criterion for evaluation will be the Mean Squared Error. Secondly, we will implement a similar set of models and algorithms to predict whether or not a school is present nearby, evaluating these models on the Correct Classification Rate. Both model selections will be carried out with a strict adherence to justify the predictions made.

Furthermore, we will discuss and compare our models based on several aspects, including uncertainty, predictive rank, and some naive predictions. The best predictive model from these will be examined in light of statistically and scientifically important covariates, along with a discussion on its limitations.

# Exploratory Data Analysis (EDA)

EDA will be performed first to understand the data structure, detect outliers and anomalies, uncover underlying patterns, and identify key variables for further analysis, thereby informing our model selection and prediction strategies.

The training data (excluding `id`) consists 5 continuous variables (`price`, `lat`, `lon`, `rate`, `lot`, and `living`), 4 binary categorical variables (`school`, `hoa`, `hvac`, and `view`), 1 ordinal variable (`year`), and 4 discrete variables (`garage`, `bath`, `bed`, and `stories`).

To ensure that R correctly recognizes the categorical variables and presents their respective summaries, it is important to explicitly indicate them using the `factor()` function before invoking the `summary()` function.

```{r, echo = FALSE}
# Specify categorical variables

train_categorical_variables <- c("id", "school", "hoa", "hvac", "view", "year")
test_categorical_variables <- c("id", "hoa", "hvac", "view", "year")

train[, train_categorical_variables] <- lapply(train[, train_categorical_variables], factor, ordered = FALSE)
train$year <- factor(train$year, ordered = TRUE)

test[, test_categorical_variables] <- lapply(test[, test_categorical_variables], factor, ordered = FALSE)
test$year <- factor(test$year, ordered = TRUE)

```

Multiple variables in the dataset exhibit missing values, as evident from the following observation. Notably, variables such as `price`, `rate`, `lot`, and `living` demonstrate skewed data distribution, which is evident from the notable difference between their respective mean and median values.

```{r, echo=FALSE}

knitr::kable(t(colSums(is.na(train))), align = "c", caption = "NA Table", format = "latex")
```

```{r, echo = FALSE}

knitr::kable(summary(train[, c("price", "lat", "lon", "rate", "garage")]), align = "c", caption = " Summary Table", format = "latex")

knitr::kable(summary(train[, c("bath", "bed", "stories", "lot", "living")]), align = "c", caption = " Summary Table", format = "latex")

```

Missing data plot below provides insights into the extent and patterns of missing values (`1.3%` Missing), aiding in understanding data completeness and will informs our data pre-processing strategies. An apparent necessity arises to perform data imputation due to the presence of missing values in multiple rows. This poses a challenge during modeling since rows with missing values are typically excluded, leading to data loss and an incomplete representation of the true data relationship within the model.

`Note : Data imputation will be done after EDA.`

```{r, echo = FALSE}

vis_miss(train)
```

Through the examination of boxplots categorized by the variable 'school,' key statistical characteristics such as central tendency, spread, and skewness of a variable can be readily discerned. Additionally, the boxplot plots enable the identification of potential outliers in the dataset.

`Note: The presented plot does not include rows with missing values, as they are excluded from the visualization.`

```{r, echo=FALSE, warning=FALSE}

b1 <- ggplot(train, aes(x = school, y = price)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b2 <- ggplot(train, aes(x = school, y = lat)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b3 <- ggplot(train, aes(x = school,  y = lon)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()
 
b4 <- ggplot(train, aes(x = school, y = rate)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b5 <- ggplot(train, aes(x = school, y = garage)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b6 <- ggplot(train, aes(x = school, y = bath)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b7 <- ggplot(train, aes(x = school, y = bed)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b8 <- ggplot(train, aes(x = school, y = stories)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b9 <- ggplot(train, aes(x = school, y = lot)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5) + 
  theme_minimal()

b10 <- ggplot(train, aes(x = school, y = living)) +
  geom_boxplot(fill = c("tomato","turquoise"), alpha = 0.5)+ 
  theme_minimal() 

grid.arrange(b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, ncol = 5, nrow = 2)
```

The multivariate plot below exhibits density plots of all the numeric variables and their correlations. Notably, medium correlations are observed between variables such as `bath`, `bed`, and `stories`. This correlation is anticipated, as houses with a greater number of rooms tend to accommodate more individuals, consequently leading to an increased number of bathrooms. Similar reasoning applies to the relationship between `stories` and `bath`. In the present dataset, it is unlikely that multicollinearity will pose a significant issue. Nevertheless, in the event that there is a need to mitigate correlation among the variables, employing transformations could prove beneficial.

`Note: The presented plot does not include rows with missing values, as they are excluded from the visualization.`

```{r, echo =FALSE, warning = FALSE}

ggpairs(train[, c('price', 'lat', 'lon', 'rate', 'garage', 'bath', 'bed', 'stories', 'lot', 'living')]) + theme_minimal()

```

The normality of variables was assessed using the Shapiro-Wilk Test, revealing that none of the variables followed a normal distribution (p-value \< 0.05). Checking for normality beforehand is important as violating this assumption can lead to poor model performance.

```{r, echo = FALSE}

knitr::kable(normality(train), align = "c", caption = "Normality Summary", format = "latex")
```

# Imputation

Imputing missing values before regression modeling is crucial. The `mice()` function is utilized, primarily for multiple imputation. However, specifying `m=1` generates a singular dataset with imputed values, resembling single imputation. This approach has employed data-specific imputation techniques, including `pmm`, `logreg`, and `polr`. With 25 iterations, most variables converge successfully. However, the `view` variable fails to converge, potentially due to high missing values, outliers, intricate patterns, or inappropriate imputation methods for its type.

```{r, echo =FALSE, include= FALSE}
old.train <- train

id <- train$id
train <- subset(train, select = -id)
temp <- mice(train, m = 1, maxit=25, seed=580)

data <- complete(temp)

data <- cbind(id, data)

id <- test$id
test <- subset(test, select = -id)
temp2 <- mice(test, m = 1, maxit= 25, seed=500)

data2 <- complete(temp2)

test <- cbind(id, data2)

set.seed(153) 
samp <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.8,0.2))

train  <- data[samp, ]
validation   <- data[!samp, ]
```

To assess potential anomalies, we will overlay density plots of the original dataset and the imputed dataset. The comparison reveals that the imputed dataset exhibits a distribution comparable to the original dataset, thereby suggesting a limited introduction of bias through the imputation process.

`Note : Only 2 plots have been shown to save space.`

```{r, echo = FALSE, warning = FALSE, message=FALSE}

d1 <- ggplot(data, aes(x = lat)) +
  geom_histogram(aes(fill = "Imputed Data"), color = "black", alpha = 0.5) +
  geom_histogram(data = old.train, aes(x = lat, fill = "Original Data"), color = "black", alpha = 0.5) + labs(title = "Density Plot", x = "lat") + theme_minimal()
  
d2 <- ggplot(data, aes(x = lon)) +
  geom_histogram(aes(fill = "Imputed Data"), color = "black", alpha = 0.5) +
  geom_histogram(data = old.train, aes(x = lon, fill = "Original Data"), color = "black", alpha = 0.5) + labs(title = "Density Plot", x = "lon") + theme_minimal()

grid.arrange(d1, d2, ncol = 2, nrow = 1)

```

In order to maintain consistency in the imputation techniques employed, we will apply similar imputation methods to the test dataset.

# Transformations

Standardized continuous variables undergo a transformation to achieve a mean of 0 and a standard deviation of 1, rendering them suitable for comparative analysis. As a result, additional transformations for standardized variables may not be necessary or advantageous. Furthermore, categorical and ordinal variables have already been transformed using the `factor()` function, leading to discrete variables. Discrete variables inherently possess a finite range of possible values without a linear or continuous relationship. Consequently, the application of transformations such as standardization or normalization may not be appropriate. Thus, no variable in the dataset requires further transformation. The next step involves commencing the modeling process.

# Modelling

## A) Predicting price (Continuous Variable)

Our analysis will commence by employing naive prediction models and subsequently progress towards state-of-the-art modeling techniques. To assess the performance of these models in predicting the variable `price`, we will utilize the evaluation metric of `Mean Squared Error`.

`Note: The training data was split into an 80:20 ratio to create a validation dataset before modeling.`

### A.1) Multiple Linear Regression

In this study, the feature selection process commenced with the utilization of `stepAIC()` followed by training a multiple linear regression (MLR) model. The mean squared error (MSE) reported herein is a result of k-fold cross-validation followed by validation dataset. By employing k-fold cross-validation alongside a validation dataset, a more comprehensive and robust evaluation of model performance is achieved, promoting generalization.

```{r, echo = FALSE, include=FALSE}

stepAIC(lm(price ~ lat + lon + rate + hoa + hvac + garage + view + year + bath + bed + stories + lot + living, data = train))
```

```{r, echo = FALSE}

k <- 10
mse_values <- numeric(k)
set.seed(13)
folds <- createFolds(train$price, k = k)

for (i in 1:k) {
  train_fold <- train[-folds[[i]], ]
  validation_fold <- train[folds[[i]], ]
  
  lm_price <- lm(formula = price ~ lon + rate + hoa + garage + 
                   year + bath + bed + stories + living,
                   data = train_fold)
  
  predictions_price_lm <- predict(lm_price, newdata = validation_fold)
  
  mse_values[i] <- mean((predictions_price_lm - validation_fold$price)^2)
}


lm_price_mse <- mean(mse_values)

predictions_price_lm <- predict(lm_price, newdata = validation)
lm_price_mse_valid <- mean((predictions_price_lm - validation$price)^2)

row1 <- data.frame("Multiple Linear Regression MSE (K-fold)", lm_price_mse)
row2 <- data.frame("Multiple Linear Regression MSE (Validation)", lm_price_mse_valid)
colnames(row1) <- c("Metric", "Value")
colnames(row2) <- c("Metric", "Value")
out_table <- rbind(row1,row2)

knitr::kable(out_table, align = "c", caption = "MSE Table", format = "latex")
```

Based on the regression coefficient summary table, it is apparent that each variable included in the model exhibits statistical significance.

```{r, echo =FALSE}
lm_price <- lm(formula = price ~ lon + rate + hoa + garage + 
                   year + bath + bed + stories + living,
                   data = train)

summary_table <- summary(lm_price)

summary_df <- as.data.frame(summary_table$coefficients)

knitr::kable(summary_df, align = "c", caption = "Linear Regression Summary", format = "latex")

```

### A.2) Ridge Regression

Ridge regression is being used because it offers several benefits that make it well-suited for predicting continuous variables. Specifically, it effectively addresses multicollinearity, balances the bias-variance tradeoff, prevents overfitting, improves robustness to outliers, and provides continuous shrinkage of coefficients. We do not have to make any transformations because we already have standardised predictors.

```{r, echo=FALSE}

x <- model.matrix(price ~ . - id - school, data = train)

y <- train$price

ridge_price <- glmnet(x, y, alpha = 0)

cv_ridge_price <-cv.glmnet(x, y, alpha = 0)

```

The first figure below shows our coefficients as a function of lambda. We can observe how the coefficients change with different values of λ, indicating the impact of regularization on their magnitudes. This provides insights into the extent of shrinkage applied to the coefficients as λ increases.

In the second plot below, we present the cross-validated MSE as a function of Log(λ). This plot helps in the selection of an appropriate λ value when predicting the variable `price`. By examining the trend of the MSE across various lambda values, we can identify the lambda that minimizes the prediction error and achieves a balance between model complexity and generalization.

```{r, echo = FALSE}

par(mfrow = c(1,2))
plot(ridge_price, xvar = "lambda", label = TRUE)
plot(cv_ridge_price)
par(mfrow = c(1,1))
```

```{r, echo = FALSE}

best.lam <- cv_ridge_price$lambda.min

predictions_price_ridge_k <- predict(ridge_price, newx = x, s=best.lam)

predictions_price_ridge_valid <- predict(ridge_price, s=best.lam, newx = model.matrix(price ~ . - id - school, data = validation))

mse_ridge_price <- mean((predictions_price_ridge_valid - validation$price)^2)
mse_ridge_price_k <- mean((predictions_price_ridge_k - y)^2)

row1 <- data.frame("Ridge Regression MSE (K-fold)", mse_ridge_price_k)
row2 <- data.frame("Ridge Regression MSE (Validation)", mse_ridge_price)
colnames(row1) <- c("Metric", "Value")
colnames(row2) <- c("Metric", "Value")
out_table <- rbind(row1,row2)

knitr::kable(out_table, align = "c", caption = "MSE Table", format = "latex")
```

### A.3) Decision Tree

Decision trees is being used to predict `price` because they can handle non-linear relationships and partition data into distinct ranges, allowing for effective regression analysis and interpretation.

The decision tree presented below offers a highly interpretable and simplified model that facilitates human understanding. Given its small size, pruning is unnecessary, as it does not require substantial computational resources for training.

```{r, echo = FALSE}

tree_price <- tree(price ~ ., data = subset(train, select = -c(id, school)))

plot(tree_price)
text(tree_price, pretty = 0)

```

The Deviance vs. Tree Size plot for decision trees provides us insights into the relationship between the complexity of the tree and the model's deviance. The deviance is a measure of the discrepancy between the predicted values of the model and the actual observed values. It is observed that, best size for our decision tree is 5.

```{r, echo = FALSE}

set.seed(125)
cv_tree_price <- cv.tree(tree_price, K = 10)
cv_data <- data.frame(size = cv_tree_price$size, dev = cv_tree_price$dev)

ggplot(data = cv_data, aes(x = size, y = dev)) +
  geom_line() +
  geom_point(shape = 21, fill = "tomato", size = 2) +
  labs(x = "Tree Size", y = "Deviance", title = "Cross-Validation Performance") +
  theme_minimal()
```

```{r, echo = FALSE}
ctrl <- tree.control(nobs = nrow(train), minsize = 5)
tree_price <- tree(price ~ ., data = subset(train, select = -c(id, school)), control = ctrl)

k <- 10
mse_values <- numeric(k)
set.seed(13)
folds <- createFolds(train$price, k = k)

for (i in 1:k) {
  train_fold <- train[-folds[[i]], ]
  validation_fold <- train[folds[[i]], ]
  ctrl_k <- tree.control(nobs = nrow(train_fold), minsize = 5)
  tree_mod <- tree(price ~ . - id -school, data = train_fold, control = ctrl_k)
  
  predictions_price_tree_k  <- predict(tree_mod, newdata = validation_fold)
  
  mse_values[i] <- mean(( predictions_price_tree_k - validation_fold$price)^2)
}

predictions_price_tree <- predict(tree_price, newdata = validation)

mse_tree_price_valid <- mean((predictions_price_tree - validation$price)^2)

mse_tree_price_k <- mean(mse_values)

row1 <- data.frame("Decision Tree MSE (K-fold)", mse_tree_price_k)
row2 <- data.frame("Decision Tree MSE (Validation)", mse_tree_price_valid)
colnames(row1) <- c("Metric", "Value")
colnames(row2) <- c("Metric", "Value")
out_table <- rbind(row1,row2)

knitr::kable(out_table, align = "c", caption = "MSE Table", format = "html")

```

### A.4) Random Forest

Random Forest is utilized for predicting continuous variables due to its ability to capture complex non-linear relationships, handle high-dimensional data effectively, and provide robustness against overfitting and outliers through ensemble averaging. It employs feature subsets to reduce correlation between trees and variance. Our initial Random Forest model includes 100 trees with an `mtry` value of 4, adhering to the general rule of setting `mtry` as the square root of the total number of predictors.

```{r, echo = FALSE}

set.seed(2)
rf_price <- randomForest(price ~ lat + lon + rate + hoa + hvac + garage + view + year + bath + bed + stories + lot + living , data = train, ntree = 100, mtry = 4)

rf_price
```

Variable importance plot indicates the relative importance of each predictor variable in the model, helping identify influential features in predicting the target variable. This can be also be used as a feature selection method for other regression methods. The `view` variable, being the least important, may have performed poorly due to non-convergence during imputation.

```{r, echo = FALSE}

importance_df <- as.data.frame(importance(rf_price))
names <- row.names(importance_df)
importance_df <- tibble::rownames_to_column(importance_df, "variable")
importance_df$variable <- names

importance_df$IncNodePurity <- as.numeric(importance_df$IncNodePurity)

ggplot(importance_df, aes(x = reorder(variable, IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  coord_flip() +
  theme_minimal() +
  xlab("Variables") +
  ylab("IncNodePurity") +
  ggtitle("Variable Importance Plot")

```

To determine the optimal model, a k-fold cross-validation technique will be employed. This involves training multiple `randomForest()` models with different configurations and subsequently selecting the model exhibiting the lowest Root Mean Square Error (RMSE). To achieve this, the `trainControl` and `expand.grid` functions will be utilized to train `randomForest()` models with `mtry` of 4, 5, 6, and 7. The evaluation metrics for all the models are presented in the following table. It is evident that model with `mtry` set to 8 performs the best.

```{r}

fitControl <- trainControl(method = "cv", number = 10)

tuneGrid <- expand.grid(mtry = c(4,5,6,7))

rf_model <- caret::train(price ~ . -id,
                         data = train,
                         method = "rf",
                         trControl = fitControl,
                         tuneGrid = tuneGrid,
                         importance = TRUE,
                         verbose = FALSE)


knitr::kable(rf_model$results, align = "c", caption = "Metric Table", format = "latex")
```

The Validation Error vs. OOB Error plot reveals the model's generalization ability and identifies the optimal number of trees for minimizing the MSE. Our analysis indicates that employing 1000 trees yields the lowest MSE and performs the best in terms of both OOB Error and Validation Error.

`Note: mtry is set to 8 based on k-fold cross-validation selection.`

```{r, echo=FALSE}

oob.err <- double(10)
test.err <- double(10)
n <- seq(100, 1000, by = 100)
for (i in 1:length(n)) {
    set.seed(2)
    fit = randomForest(price ~ lat + lon + rate + hoa + hvac + garage + view + year + bath + bed + stories + lot + living, data = train, mtry = 8, 
        ntree = n[[i]]) 
    oob.err[i] = fit$mse[n[[i]]]
    pred = predict(fit, data[!samp,])
    test.err[i] = with(data[!samp,], mean((pred - price)^2))
}

df <- data.frame(ntree = n, test.err = test.err, oob.err = oob.err)

ggplot(df) +
  geom_point(aes(x = ntree, y = test.err, color = "Validation Error"), shape = 19) +
  geom_line(aes(x = ntree, y = test.err, color = "Validation Error")) +
  geom_point(aes(x = ntree, y = oob.err, color = "OOB Error"), shape = 19) +
  geom_line(aes(x = ntree, y = oob.err, color = "OOB Error")) +
  labs(y = "Mean Squared Error", color = "Error Type") +
  scale_color_manual(values = c("Validation Error" = "blue", "OOB Error" = "red"))

```

This culminates in our ultimate choice of utilizing the `randomForest()` algorithm with 1000 trees and `mtry` set to 8. The Mean Squared Error evaluated through k-fold validation as well as the separate Validation set is demonstrated below.

```{r, echo = FALSE}

fitControl <- trainControl(method = "cv", number = 10)

tuneGrid <- expand.grid(mtry = 8)

rf_model <- caret::train(price ~ . -id,
                         data = train,
                         method = "rf",
                         trControl = fitControl,
                         tuneGrid = tuneGrid,
                         importance = TRUE,
                         ntree = 1000,
                         verbose = FALSE)

rmse <- rf_model$results
rmse <- min(rmse$RMSE)
mse_rf_price_k <- rmse^2
mse_rf_price_valid <- min(test.err)

row1 <- data.frame("Random Forest MSE (K-fold)", mse_rf_price_k)
row2 <- data.frame("Random Forest MSE (Validation)", mse_rf_price_valid)
colnames(row1) <- c("Metric", "Value")
colnames(row2) <- c("Metric", "Value")
out_table <- rbind(row1,row2)

knitr::kable(out_table, align = "c", caption = "MSE Table", format = "html")

rf_price <- randomForest(price ~ lat + lon + rate + hoa + hvac + garage + view + year + bath + bed + stories + lot + living , data = train, ntree = 1000, mtry = 8)

```


### A.5) Boosting

Boosting is effective for predicting continuous variables by combining weak models to capture complex relationships. Our initial boosting model consisted of 5000 trees and an interaction depth of 4.

```{r, echo = FALSE}

set.seed(249)
boost_price <- gbm(price ~ lat + lon + rate + hoa + hvac + garage + view +
                  year + bath + bed + stories + lot + living, data =train,
                  distribution = 'gaussian', n.trees = 5000,
                  interaction.depth = 4, shrinkage = 0.01)

```

Similar to Random Forest, variable importance plots can be generated to assess the significance of predictor features.

```{r, echo = FALSE}

boost_summary <- summary(boost_price, plot = FALSE)

boost_df <- data.frame(boost_summary[1:13,])

ggplot(boost_df, aes(x = boost_df[,2], y = reorder(boost_df[,1], -boost_df[,2], decreasing = TRUE))) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  theme_minimal() +
  ylab("Variables") +
  xlab("Relative Influence") +
  ggtitle("Variable Importance Plot")
```

To determine the optimal model, a k-fold cross-validation technique will be employed. This involves training multiple `gbm()` models with different configurations and subsequently selecting the model exhibiting the lowest Root Mean Square Error (RMSE). To achieve this, the `trainControl` and `expand.grid` functions will be utilized to train `gbm()` models with sizes of 1000, 3000, and 5000, while considering interaction depths of 4, 6, and 8. The evaluation metrics for all the models are presented in the following table. It is evident that the model with 1000 trees and an interaction depth of 8 demonstrates superior performance compared to the other models.

```{r}

fitControl <- trainControl(method = "cv", number = 10)

tuneGrid <- expand.grid(n.trees = seq(1000, 5000, by = 2000),
                        interaction.depth = c(4,6,8),
                        shrinkage = 0.01,
                        n.minobsinnode = 10)

gbm_model <- caret::train(price ~ . -id,
                          data = train,
                          method = "gbm",
                          trControl = fitControl,
                          tuneGrid = tuneGrid,
                          distribution = "gaussian", 
                          verbose = FALSE)

knitr::kable(gbm_model$results, align = "c", caption = "Metric Table", format = "latex")
```

The plot validates our model's expected behavior, with the MSE consistently decreasing as the number of trees increases. This improvement occurs due to Boosting's iterative learning process, which corrects and enhances the model's performance over time.

```{r, echo=FALSE}

boost_price <- gbm(price ~ lat + lon + rate + hoa + hvac + garage + view +
                  year + bath + bed + stories + lot + living, data =train,
                  distribution = 'gaussian', n.trees = 1000,
                  interaction.depth = 8, shrinkage = 0.01)

n.trees <- seq(from = 10, to = 1000, by = 10)
predmat <- predict(boost_price, newdata = validation, n.trees = n.trees)

berr <- with(validation, apply((predmat - validation$price)^2, 2, mean))
plot(n.trees, berr, pch = 19, ylab = "Mean Squared Error", xlab = "Trees", 
    main = "Boosting Validation Error")
abline(h = min(test.err), col = "red")

```

```{r, echo=FALSE}

rmse <- gbm_model$results
rmse <- min(rmse$RMSE)
mse_gbm_price_k <-  rmse^2
mse_gbm_price_valid <- min(test.err)
row1 <- data.frame("Boosting MSE (K-fold)",mse_gbm_price_k)
row2 <- data.frame("Boosting MSE (Validation)", mse_gbm_price_valid)
colnames(row1) <- c("Metric", "Value")
colnames(row2) <- c("Metric", "Value")
out_table <- rbind(row1,row2)

knitr::kable(out_table, align = "c", caption = "MSE Table", format = "latex")
```


### Discussion 

Having constructed our models, we will now predict our 50% test data using all the models and analyze the Mean Squared Error (MSE) statistics for K-fold, Validation, and Test. These statistics serve as indicators of how well our models generalize to unseen data. Based on established findings, our random forest models demonstrate superior prediction accuracy when applied to 50% of the test data. In light of this empirical evidence, we will designate the random forest model as our optimal choice for the initial prediction component.

```{r, echo = FALSE, include=FALSE}

# Multiple Linear Regression Prediction

kaggle_price_lm <- predict(lm_price, newdata = test)

kaggle_matrix_price_lm <- cbind(as.numeric(levels(test$id))[test$id], kaggle_price_lm)

colnames(kaggle_matrix_price_lm) <- c("id", "price")

write.csv(kaggle_matrix_price_lm, "kaggle_price_lm.csv", row.names = FALSE)


# Ridge Prediction

x <- model.matrix( ~ . -id ,data = test)

kaggle_price_ridge <- predict(ridge_price, s = best.lam, newx = x)

kaggle_matrix_price_ridge <- cbind(as.numeric(levels(test$id))[test$id], kaggle_price_ridge)

colnames(kaggle_matrix_price_ridge) <- c("id", "price")

write.csv(kaggle_matrix_price_ridge, "kaggle_price_ridge.csv", row.names = FALSE)

# Decision Tree Prediction

kaggle_price_tree <- predict(tree_price, newdata = test)

kaggle_matrix_price_tree <- cbind(as.numeric(levels(test$id))[test$id], kaggle_price_tree)

colnames(kaggle_matrix_price_tree) <- c("id", "price")

write.csv(kaggle_matrix_price_tree, "kaggle_price_tree.csv", row.names = FALSE)

# Random Forest Prediction

kaggle_price_rf <- predict(rf_price, newdata = test)

kaggle_matrix_price_rf <- cbind(as.numeric(levels(test$id))[test$id], kaggle_price_rf)

colnames(kaggle_matrix_price_rf) <- c("id", "price")

write.csv(kaggle_matrix_price_rf, "kaggle_price_rf.csv", row.names = FALSE)

# Boosting Prediction

kaggle_price_boost <- predict(boost_price, newdata = test)

kaggle_matrix_price_boost <- cbind(as.numeric(levels(test$id))[test$id], kaggle_price_boost)

colnames(kaggle_matrix_price_boost) <- c("id", "price")

write.csv(kaggle_matrix_price_boost, "kaggle_price_boost.csv", row.names = FALSE)

```

```{r, echo = FALSE}

models <- c("Multiple Linear Regression", "Ridge Regression", "Decision Tree", "Random Forest", "Boosting")

mse_k <- c(lm_price_mse, mse_ridge_price_k, mse_tree_price_k, mse_ridge_price_k, mse_gbm_price_k)

mse_valid <- c(lm_price_mse_valid, mse_ridge_price, mse_tree_price_valid, mse_ridge_price, mse_gbm_price_valid)

mse_test <- c(0.43900, 0.44807, 0.63099, 0.29468, 0.31265)

df <- data.frame(Model = models, MSE_K_Fold = mse_k, MSE_Validation = mse_valid, MSE_Test = mse_test)

colnames(df) <- c("Model", "K-fold MSE", "Validation MSE", "Test MSE")

```

Performing an analysis to assess the fulfillment or violation of assumptions within our model is of utmost importance.